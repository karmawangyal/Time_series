{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== holidays_events ===\n",
      "\n",
      ".info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   date         350 non-null    object\n",
      " 1   type         350 non-null    object\n",
      " 2   locale       350 non-null    object\n",
      " 3   locale_name  350 non-null    object\n",
      " 4   description  350 non-null    object\n",
      " 5   transferred  350 non-null    bool  \n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 14.1+ KB\n",
      "None\n",
      "\n",
      ".head():\n",
      "         date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
      "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n",
      "\n",
      "=== train ===\n",
      "\n",
      ".info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   id           int64  \n",
      " 1   date         object \n",
      " 2   store_nbr    int64  \n",
      " 3   family       object \n",
      " 4   sales        float64\n",
      " 5   onpromotion  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 137.4+ MB\n",
      "None\n",
      "\n",
      ".head():\n",
      "   id        date  store_nbr      family  sales  onpromotion\n",
      "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1       BOOKS    0.0            0\n",
      "\n",
      "=== stores ===\n",
      "\n",
      ".info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54 entries, 0 to 53\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   store_nbr  54 non-null     int64 \n",
      " 1   city       54 non-null     object\n",
      " 2   state      54 non-null     object\n",
      " 3   type       54 non-null     object\n",
      " 4   cluster    54 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.2+ KB\n",
      "None\n",
      "\n",
      ".head():\n",
      "   store_nbr           city                           state type  cluster\n",
      "0          1          Quito                       Pichincha    D       13\n",
      "1          2          Quito                       Pichincha    D       13\n",
      "2          3          Quito                       Pichincha    D        8\n",
      "3          4          Quito                       Pichincha    D        9\n",
      "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read CSV normally\n",
    "def read_csv_clean(file_path):\n",
    "    df = pd.read_csv(file_path)  # no need to drop anything\n",
    "    return df\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    \"/Users/karma/Desktop/store-sales-time-series-forecasting/holidays_events.csv\",\n",
    "    \"/Users/karma/Desktop/store-sales-time-series-forecasting/train.csv\",\n",
    "    \"/Users/karma/Desktop/store-sales-time-series-forecasting/stores.csv\"\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read each CSV and print info + head\n",
    "for file in csv_files:\n",
    "    name = os.path.basename(file).split('.')[0]\n",
    "    df = read_csv_clean(file)\n",
    "    dataframes[name] = df\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"\\n.info():\")\n",
    "    print(df.info())\n",
    "    print(\"\\n.head():\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type does not affect the sales so I will drop one to avoid complications.\n",
    "dataframes['stores'] = dataframes['stores'].drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state  cluster\n",
       "0          1          Quito                       Pichincha       13\n",
       "1          2          Quito                       Pichincha       13\n",
       "2          3          Quito                       Pichincha        8\n",
       "3          4          Quito                       Pichincha        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas        4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[\"stores\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date columns are datetime\n",
    "for df_name in ['train', 'holidays_events']:\n",
    "    dataframes[df_name]['date'] = pd.to_datetime(dataframes[df_name]['date'])\n",
    "\n",
    "# Merge train with stores\n",
    "df_combined = pd.merge(\n",
    "    dataframes['train'], \n",
    "    dataframes['stores'], \n",
    "    on='store_nbr', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with holidays_events\n",
    "df_combined = pd.merge(\n",
    "    df_combined, \n",
    "    dataframes['holidays_events'], \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>cluster</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>National</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Primer dia del ano</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>National</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Primer dia del ano</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>National</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Primer dia del ano</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>National</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Primer dia del ano</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>13</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>National</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Primer dia del ano</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date  store_nbr      family  sales  onpromotion   city      state  \\\n",
       "0   0 2013-01-01          1  AUTOMOTIVE    0.0            0  Quito  Pichincha   \n",
       "1   1 2013-01-01          1   BABY CARE    0.0            0  Quito  Pichincha   \n",
       "2   2 2013-01-01          1      BEAUTY    0.0            0  Quito  Pichincha   \n",
       "3   3 2013-01-01          1   BEVERAGES    0.0            0  Quito  Pichincha   \n",
       "4   4 2013-01-01          1       BOOKS    0.0            0  Quito  Pichincha   \n",
       "\n",
       "   cluster     type    locale locale_name         description transferred  \n",
       "0       13  Holiday  National     Ecuador  Primer dia del ano       False  \n",
       "1       13  Holiday  National     Ecuador  Primer dia del ano       False  \n",
       "2       13  Holiday  National     Ecuador  Primer dia del ano       False  \n",
       "3       13  Holiday  National     Ecuador  Primer dia del ano       False  \n",
       "4       13  Holiday  National     Ecuador  Primer dia del ano       False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "date                 0\n",
       "store_nbr            0\n",
       "family               0\n",
       "sales                0\n",
       "onpromotion          0\n",
       "city                 0\n",
       "state                0\n",
       "cluster              0\n",
       "type           2551824\n",
       "locale         2551824\n",
       "locale_name    2551824\n",
       "description    2551824\n",
       "transferred    2551824\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred value counts:\n",
      "transferred\n",
      "NaN      2551824\n",
      "False     486486\n",
      "True       16038\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3054348 entries, 0 to 3054347\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   id           int64         \n",
      " 1   date         datetime64[ns]\n",
      " 2   sales        float64       \n",
      " 3   onpromotion  int64         \n",
      " 4   day_off      bool          \n",
      " 5   day          int32         \n",
      " 6   month        int32         \n",
      " 7   weekday      int32         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(1), int32(3), int64(2)\n",
      "memory usage: 131.1 MB\n",
      "None\n",
      "\n",
      "Final dataset preview:\n",
      "              id       date     sales  onpromotion  day_off  day  month  \\\n",
      "3054343  3000883 2017-08-15   438.133            0     True   15      8   \n",
      "3054344  3000884 2017-08-15   154.553            1     True   15      8   \n",
      "3054345  3000885 2017-08-15  2419.729          148     True   15      8   \n",
      "3054346  3000886 2017-08-15   121.000            8     True   15      8   \n",
      "3054347  3000887 2017-08-15    16.000            0     True   15      8   \n",
      "\n",
      "         weekday  \n",
      "3054343        1  \n",
      "3054344        1  \n",
      "3054345        1  \n",
      "3054346        1  \n",
      "3054347        1  \n"
     ]
    }
   ],
   "source": [
    "#I want to work on type(working/holiday) because generally this affects a lot on sales.\n",
    "#Transferred column makes my analysis a bit complicate, I will see how many True values are present and decide to keep it remove it.\n",
    "# ----------------------------\n",
    "# 1. Inspect 'transferred' column\n",
    "# ----------------------------\n",
    "print(\"Transferred value counts:\")\n",
    "print(df_combined[\"transferred\"].value_counts(dropna=False))\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Create 'day_off' based on holiday/event type\n",
    "# ----------------------------\n",
    "day_off_categories = [\"Holiday\", \"Event\", \"Additional\", \"Bridge\"]\n",
    "df_combined[\"day_off\"] = df_combined[\"type\"].isin(day_off_categories)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Copy combined DataFrame\n",
    "# ----------------------------\n",
    "df_final = df_combined.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Drop unnecessary columns\n",
    "# ----------------------------\n",
    "drop_cols = [\n",
    "    \"store_nbr\", \"family\", \"city\", \"state\", \"cluster\",\n",
    "    \"type\", \"locale\", \"locale_name\", \"description\", \"transferred\"\n",
    "]\n",
    "df_final.drop(columns=drop_cols, inplace=True, errors='ignore')  # errors='ignore' if column missing\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Add date features\n",
    "# ----------------------------\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "df_final['day'] = df_final['date'].dt.day\n",
    "df_final['month'] = df_final['date'].dt.month\n",
    "df_final['weekday'] = df_final['date'].dt.weekday\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Check final dataset\n",
    "# ----------------------------\n",
    "print(\"\\nFinal dataset info:\")\n",
    "print(df_final.info())\n",
    "print(\"\\nFinal dataset preview:\")\n",
    "print(df_final.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date    total_sales  onpromotion  day_off  day  month  weekday\n",
      "0  2013-01-01    2511.618999            0        1    1      1        1\n",
      "1  2013-01-02  496092.417944            0        0    2      1        2\n",
      "2  2013-01-03  361461.231124            0        0    3      1        3\n",
      "3  2013-01-04  354459.677093            0        0    4      1        4\n",
      "4  2013-01-05  477350.121229            0        0    5      1        5\n",
      "5  2013-01-06  519695.401088            0        0    6      1        6\n",
      "6  2013-01-07  336122.801066            0        0    7      1        0\n",
      "7  2013-01-08  318347.777981            0        0    8      1        1\n",
      "8  2013-01-09  302530.809018            0        0    9      1        2\n",
      "9  2013-01-10  258982.003049            0        0   10      1        3\n",
      "10 2013-01-11  289737.685085            0        0   11      1        4\n",
      "11 2013-01-12  403258.212011            0        0   12      1        5\n",
      "12 2013-01-13  464638.547998            0        0   13      1        6\n",
      "13 2013-01-14  293348.362078            0        0   14      1        0\n",
      "14 2013-01-15  299129.549954            0        0   15      1        1\n",
      "15 2013-01-16  318347.913946            0        0   16      1        2\n",
      "16 2013-01-17  267498.515975            0        0   17      1        3\n",
      "17 2013-01-18  296130.850028            0        0   18      1        4\n",
      "18 2013-01-19  432459.852021            0        0   19      1        5\n",
      "19 2013-01-20  461092.386047            0        0   20      1        6\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sales, promotions, and day_off per id per date\n",
    "df_daily = df_final.groupby(['date']).agg({\n",
    "    'sales': 'sum',          # total sales per id per date\n",
    "    'onpromotion': 'sum',    # total promotions per id per date\n",
    "    'day_off': 'max',        # if any row is day_off, mark as 1\n",
    "    'day': 'first',          # keep day\n",
    "    'month': 'first',        # keep month\n",
    "    'weekday': 'first'       # keep weekday\n",
    "}).reset_index()\n",
    "\n",
    "# Rename column\n",
    "df_daily.rename(columns={'sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Convert day_off to integer\n",
    "df_daily['day_off'] = df_daily['day_off'].astype(int)\n",
    "\n",
    "# Sort by id and date\n",
    "df_daily = df_daily.sort_values(['date']).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(df_daily.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date           0\n",
       "total_sales    0\n",
       "onpromotion    0\n",
       "day_off        0\n",
       "day            0\n",
       "month          0\n",
       "weekday        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make sure to sort the dataframe on date, so that it can be fed to LSTM.\n",
    "df_daily = df_daily.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "features = ['onpromotion', 'day_off', 'day', 'month', 'weekday']\n",
    "target = 'total_sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode boolean 0/1\n",
    "df_daily['day_off'] = df_daily['day_off'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features and target to make sure biasness is avoided against smaller numbers. Also, activation fn used in LSTM outputs values between 0 to 1 or -1 to 1, so helps to converge faster by using scaler.\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(df_daily[features])\n",
    "y_scaled = scaler_y.fit_transform(df_daily[[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_lstm shape: (1654, 30, 5)\n",
      "y_lstm shape: (1654, 1)\n"
     ]
    }
   ],
   "source": [
    "#Create a sequence that is acceptable to LSTM nn (3D input(samples, timesteps, features))\n",
    "def create_sequences(X, y, seq_length=30):\n",
    "    \"\"\"\n",
    "    Create sequences of features and target for LSTM\n",
    "    seq_length: number of past days to use for prediction\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "SEQ_LENGTH = 30  # use past 30 days to predict next day\n",
    "X_lstm, y_lstm = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)\n",
    "\n",
    "print(\"X_lstm shape:\", X_lstm.shape)\n",
    "print(\"y_lstm shape:\", y_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTMs learn from the ordered sequence of past values instead of treating each day as independent.\n",
    "#Therefore, we have 1654 training sequences of 30 days each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:  30 past days of features ‚Üí Output: sales on day 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Apple MPS: mps\n",
      "Epoch [1/50] | Train Loss: 0.0157 | Val Loss: 0.0162\n",
      "Epoch [2/50] | Train Loss: 0.0059 | Val Loss: 0.0087\n",
      "Epoch [3/50] | Train Loss: 0.0054 | Val Loss: 0.0091\n",
      "Epoch [4/50] | Train Loss: 0.0051 | Val Loss: 0.0076\n",
      "Epoch [5/50] | Train Loss: 0.0049 | Val Loss: 0.0062\n",
      "Epoch [6/50] | Train Loss: 0.0045 | Val Loss: 0.0055\n",
      "Epoch [7/50] | Train Loss: 0.0041 | Val Loss: 0.0078\n",
      "Epoch [8/50] | Train Loss: 0.0041 | Val Loss: 0.0086\n",
      "Epoch [9/50] | Train Loss: 0.0039 | Val Loss: 0.0084\n",
      "Epoch [10/50] | Train Loss: 0.0037 | Val Loss: 0.0081\n",
      "Epoch [11/50] | Train Loss: 0.0036 | Val Loss: 0.0089\n",
      "Epoch [12/50] | Train Loss: 0.0036 | Val Loss: 0.0061\n",
      "Epoch [13/50] | Train Loss: 0.0036 | Val Loss: 0.0060\n",
      "‚èπÔ∏è Early stopping at epoch 13\n",
      "‚úÖ Best model loaded from checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# Device setup\n",
    "# =============================\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"‚úÖ Using Apple MPS:\", device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"‚úÖ Using NVIDIA GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU only\")\n",
    "\n",
    "# =============================\n",
    "# Example LSTM model\n",
    "# =============================\n",
    "class SalesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(SalesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]     # last timestep output\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# =============================\n",
    "# Early Stopping\n",
    "# =============================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, save_path=\"best_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.save_path)  # Save best weights\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# =============================\n",
    "# Hyperparameters\n",
    "# =============================\n",
    "input_size = 5      # number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# =============================\n",
    "# Train/Validation split\n",
    "# (Assumes X_lstm, y_lstm already created)\n",
    "# =============================\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_lstm, y_lstm, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor   = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# Define model, loss, optimizer\n",
    "# =============================\n",
    "model = SalesLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=1e-4, save_path=\"best_lstm_model.pth\")\n",
    "\n",
    "# =============================\n",
    "# TensorBoard Writer\n",
    "# =============================\n",
    "log_dir = \"runs/sales_lstm\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# =============================\n",
    "# Training Loop\n",
    "# =============================\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # ‚úÖ Log to TensorBoard\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": avg_train_loss, \"Validation\": avg_val_loss}, epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# =============================\n",
    "# Load Best Model After Training\n",
    "# =============================\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "print(\"‚úÖ Best model loaded from checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded for forecasting\n"
     ]
    }
   ],
   "source": [
    "#model structure\n",
    "model = SalesLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Load saved weights\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "model.eval()  # set to evaluation mode\n",
    "print(\"‚úÖ Model loaded for forecasting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_lstm shape: (1654, 30, 5)\n",
      "y_lstm shape: (1654, 1)\n"
     ]
    }
   ],
   "source": [
    "#Lets load the saved model for forecasting\n",
    "#first prepare the input as expected by the model\n",
    "print(\"X_lstm shape:\", X_lstm.shape)\n",
    "print(\"y_lstm shape:\", y_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 5)\n",
      "torch.Size([1, 30, 5])\n"
     ]
    }
   ],
   "source": [
    "#take the last sequence from X_lstm\n",
    "last_sequence = X_lstm[-1]\n",
    "print(last_sequence.shape)\n",
    "last_sequence = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "# Add batch dimension: (1, seq_length, input_size)\n",
    "last_sequence = last_sequence.unsqueeze(0)\n",
    "\n",
    "print(last_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next day scaled prediction: 0.22807924449443817\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    next_day_scaled = model(last_sequence)\n",
    "print(\"Next day scaled prediction:\", next_day_scaled.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next day forecast (original scale): 749544.3\n"
     ]
    }
   ],
   "source": [
    "# Convert prediction to numpy\n",
    "next_day_scaled_np = next_day_scaled.cpu().numpy()  # shape (1,1)\n",
    "\n",
    "# Inverse transform using scaler_y\n",
    "next_day = scaler_y.inverse_transform(next_day_scaled_np)\n",
    "\n",
    "print(\"Next day forecast (original scale):\", next_day[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
